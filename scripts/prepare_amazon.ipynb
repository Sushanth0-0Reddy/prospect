{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Amazon Reviews Dataset\n",
    "\n",
    "In this notebook, we:\n",
    "1. Download the WILDS Amazon Reviews benchmark dataset.\n",
    "2. Use a subset of 10,000 to fine-tune a BERT model with the empirical risk minimization objective.\n",
    "3. Apply the network to another held-out training sample of 10,000 examples, as well as validation and test sets of size 10,000 each.\n",
    "4. Use the penultimate layer of the network outputs to generate frozen feature representations of the train, validation, and test sets described above.\n",
    "\n",
    "The step requires the use of a GPU to run efficiently, listed as the `DEVICE` parameter below. \n",
    "Before running this notebook, please read the environment setup instructions in `README.md`. Then, you may download the `wilds` and `transformer` packages using the code below, and restarting the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install wilds\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune BERT Model\n",
    "\n",
    "We first create the fine-tuning set to use on the pre-trained BERT architecture. This involves downloading, splitting, and tokenizing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "ADAMW_TOLERANCE = 1e-8\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "SEED = 123\n",
    "FILENAME = \"bert_amazon_finetuned\"\n",
    "DEVICE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on 'cuda:1'.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = f\"cuda:{DEVICE}\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(\"Running on '%s'.\" % device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset download takes (on the order of) 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset, and download it if necessary.\n",
    "dataset = get_dataset(dataset=\"amazon\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(split, n, tokenizer):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attn_masks = []\n",
    "    labels = []\n",
    "    metadata = []\n",
    "\n",
    "    # For every sentence...\n",
    "    # sentences = dataset[split][\"text\"]\n",
    "    # train_data = dataset.get_subset(split)\n",
    "    loader = get_train_loader(\"standard\", dataset.get_subset(split), batch_size=1)\n",
    "    # if split != \"train\":\n",
    "    #     idx = np.arange(len(sentences))\n",
    "    for i, (x, y, z) in tqdm(enumerate(loader)):\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            x[0],  # Sentence to encode.\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=80,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask = True,   # Construct attn. masks.\n",
    "            return_tensors=\"pt\",  # Return pytorch tensors.\n",
    "        )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_dict[\"input_ids\"])\n",
    "        attn_masks.append(encoded_dict[\"attention_mask\"])\n",
    "        labels.append(y.item())\n",
    "        metadata.append(z)\n",
    "\n",
    "        if i == n - 1:\n",
    "            break\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attn_masks = torch.cat(attn_masks, dim=0)\n",
    "    labels = torch.tensor(labels).long()\n",
    "    metadata = torch.cat(metadata)\n",
    "\n",
    "    return input_ids, attn_masks, labels, metadata\n",
    "\n",
    "class Amazon(Dataset):\n",
    "    def __init__(self, input_ids, attn_masks, labels, metadata):\n",
    "        self.input_ids = input_ids\n",
    "        self.attn_masks = attn_masks\n",
    "        self.labels = labels\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.input_ids[i], self.attn_masks[i], self.labels[i], self.metadata[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloaders for the non-convex training set (`train`) and convex training set (`finetune`) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders.\n",
    "np.random.seed(123)\n",
    "n = 20000\n",
    "finetune_idx = np.random.choice(n, 10000, replace=False)\n",
    "train_idx = np.delete(np.arange(n), finetune_idx)\n",
    "print(np.intersect1d(finetune_idx, train_idx))\n",
    "print(len(np.unique(np.union1d(finetune_idx, train_idx))))\n",
    "\n",
    "input_ids, attn_masks, labels, metadata = get_split(\"train\", n, tokenizer)\n",
    "\n",
    "train_dataset = Amazon(input_ids[train_idx], attn_masks[train_idx], labels[train_idx], metadata[train_idx])\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, sampler=RandomSampler(train_dataset), batch_size=BATCH_SIZE\n",
    ")\n",
    "print(\"{:>5,} training samples.\".format(len(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Validation and Test Set\n",
    "\n",
    "Next, we apply the same method to tokenize and create dataloaders for the validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Amazon(*get_split(\"val\", 10000, tokenizer))\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset, sampler=SequentialSampler(val_dataset), batch_size=BATCH_SIZE\n",
    ")\n",
    "print(\"{:>5,} validation samples.\".format(len(val_dataset)))\n",
    "\n",
    "test_dataset = Amazon(*get_split(\"test\", 10000, tokenizer))\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, sampler=RandomSampler(test_dataset), batch_size=BATCH_SIZE\n",
    ")\n",
    "print(\"{:>5,} test samples.\".format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model \n",
    "\n",
    "We run the standard training loop for `N_EPOCHS` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=5,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def format_time(elapsed):\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = LEARNING_RATE, eps = ADAMW_TOLERANCE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = EPOCHS * BATCH_SIZE * len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full training loop is run below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything.\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "for epoch_i in range(EPOCHS):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"======== Epoch {:} / {:} ========\".format(epoch_i + 1, EPOCHS))\n",
    "    print(\"Training...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(\n",
    "                \"  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.\".format(\n",
    "                    step, len(train_dataloader), elapsed\n",
    "                )\n",
    "            )\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        output = model(\n",
    "            b_input_ids,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "        \n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # TODO: See if this is needed.\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            \"epoch\": epoch_i + 1,\n",
    "            \"Training Loss\": avg_train_loss,\n",
    "            \"Valid. Loss\": avg_val_loss,\n",
    "            \"Valid. Accur.\": avg_val_accuracy,\n",
    "            \"Training Time\": training_time,\n",
    "            \"Validation Time\": validation_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\n",
    "    \"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0))\n",
    ")\n",
    "\n",
    "# Save the model.\n",
    "torch.save(model, f\"{FILENAME}.pt\")\n",
    "pickle.dump(\n",
    "    training_stats, open(f\"training_stats_{FILENAME}.p\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Embeddings\n",
    "\n",
    "Next, we visualize the learned last-layer embeddings to observe separations between classes, indicating that they have picked up the required signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "model = torch.load(f\"{FILENAME}.pt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states(dataloader):\n",
    "    model.eval()\n",
    "    hidden_states = []\n",
    "    labels = []\n",
    "    metadata = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_metadata = batch[3]\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            hidden_states.append(torch.mean(output.hidden_states[-1], dim=1))\n",
    "            labels.append(b_labels)\n",
    "            metadata.append(b_metadata)\n",
    "    return torch.cat(hidden_states, dim=0).cpu().numpy(), torch.cat(labels, dim=0).cpu().numpy(), torch.cat(metadata, dim=0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = Amazon(input_ids[finetune_idx], attn_masks[finetune_idx], labels[finetune_idx], metadata[finetune_idx])\n",
    "split_dataloader = DataLoader(\n",
    "    split_dataset, sampler=RandomSampler(split_dataset), batch_size=BATCH_SIZE\n",
    ")\n",
    "print(\"{:>5,} fine-tuning samples.\".format(len(split_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, z_train = get_hidden_states(split_dataloader)\n",
    "x_val, y_val, z_val = get_hidden_states(validation_dataloader)\n",
    "x_test, y_test, z_test = get_hidden_states(test_dataloader)\n",
    "\n",
    "x_train = x_train / np.linalg.norm(x_train, axis=1)[:, None]\n",
    "x_val = x_val / np.linalg.norm(x_val, axis=1)[:, None]\n",
    "x_test = x_test / np.linalg.norm(x_test, axis=1)[:, None]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "x = PCA(n_components=2).fit_transform(x_train)\n",
    "\n",
    "ax.scatter(x[:, 0], x[:, 1], c=y_train, s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce the dimension of the embeddings using principle components analysis (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = x_train.shape[1]\n",
    "pca = PCA(n_components=embed_dim).fit(x_train)\n",
    "\n",
    "threshold = 0.9\n",
    "cumsum = 0.0\n",
    "for i, eig in enumerate(pca.explained_variance_ratio_):\n",
    "    cumsum += eig\n",
    "    if cumsum >= threshold:\n",
    "        n_components = i + 1\n",
    "        print(f\"{cumsum} variance explained by {n_components}/{embed_dim} components.\")\n",
    "        break\n",
    "\n",
    "X_train = pca.transform(x_train)[:, 0:n_components]\n",
    "X_val   = pca.transform(x_val)[:, 0:n_components]\n",
    "X_test  = pca.transform(x_test)[:, 0:n_components]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the resulting output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"amazon\"\n",
    "os.makedirs(f\"../data/{dataset}\", exist_ok=True)\n",
    "\n",
    "np.save(f\"../data/{dataset}/X_train.npy\", X_train)\n",
    "np.save(f\"../data/{dataset}/y_train.npy\", y_train)\n",
    "np.save(f\"../data/{dataset}/z_train.npy\", z_train)\n",
    "np.save(f\"../data/{dataset}/X_val.npy\", X_val)\n",
    "np.save(f\"../data/{dataset}/y_val.npy\", y_val)\n",
    "np.save(f\"../data/{dataset}/z_val.npy\", z_val)\n",
    "np.save(f\"../data/{dataset}/X_test.npy\", X_test)\n",
    "np.save(f\"../data/{dataset}/y_test.npy\", y_test)\n",
    "np.save(f\"../data/{dataset}/z_test.npy\", z_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
